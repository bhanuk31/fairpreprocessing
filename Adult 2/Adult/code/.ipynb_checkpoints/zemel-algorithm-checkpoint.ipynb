{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import scipy.optimize as optim\n",
    "from helpers import *\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ZemelTransform(encoded_data):\n",
    "    \"\"\"\n",
    "    This function learns the model from the training set for Adult data which is binarized (in the steps below) and passed to the LBFGS solver\n",
    "    after computing the composite function using helpers.py. helpers.py has all the functions used in Zemel's paper as a s\n",
    "    separate module. As given in the paper the parameter setting (0.01,1,50) with\n",
    "    a k=10 gives good accuracy and reasonable discrimination for Adult dataset. Another setting of (0.01,0.01,0) gives low\n",
    "    discrimination. k value can be modified in the code below or passed as a parameter.\n",
    "    \"\"\"\n",
    "    k=5\n",
    "    df_train=encoded_data\n",
    "    catlabelsage=pd.get_dummies(encoded_data['Age (decade)'])\n",
    "    catlabelseducation=pd.get_dummies(encoded_data['Education Years'])\n",
    "    encoded_data=pd.concat([encoded_data,catlabelsage],axis=1)\n",
    "    encoded_data=encoded_data.drop(['Age (decade)'],axis=1)\n",
    "    encoded_data=pd.concat([encoded_data,catlabelseducation],axis=1)\n",
    "    encoded_data=encoded_data.drop(['Education Years'],axis=1)\n",
    "    encoded_data=encoded_data.drop(['Income'],axis=1)\n",
    "    encoders = {}\n",
    "    encoders['Gender'] = preprocessing.LabelEncoder()\n",
    "    encoded_data['Gender'] = encoders['Gender'].fit_transform(encoded_data['Gender'])\n",
    "    sensitive_idx = np.array(np.where(encoded_data['Gender']==0))[0].flatten()\n",
    "    nonsensitive_idx = np.array(np.where(encoded_data['Gender']!=0))[0].flatten()\n",
    "    original_sensitive=df_train.iloc[sensitive_idx,:]\n",
    "    original_nonsensitive=df_train.iloc[nonsensitive_idx,:]\n",
    "    originaldat=pd.concat((original_sensitive,original_nonsensitive))\n",
    "    ytrain_sensitive=np.array(encoded_data['Income Binary'])[sensitive_idx]\n",
    "    ytrain_nonsensitive=np.array(encoded_data['Income Binary'])[nonsensitive_idx]\n",
    "    encoded_data=encoded_data.drop('Income Binary',axis=1)\n",
    "    traindat=np.array(encoded_data)\n",
    "    training_sensitive=traindat[sensitive_idx,:]\n",
    "    training_nonsensitive=traindat[nonsensitive_idx,:]\n",
    "    rez = np.random.uniform(size=traindat.shape[1] * 2 + k + traindat.shape[1] * k)\n",
    "    bnd = []\n",
    "    for i, k2 in enumerate(rez):\n",
    "        if i < traindat.shape[1] * 2 or i >= traindat.shape[1] * 2 + k:\n",
    "            bnd.append((None, None))\n",
    "        else:\n",
    "            bnd.append((0, 1))\n",
    "    rez = optim.fmin_l_bfgs_b(LFR, x0=rez, epsilon=1e-5, \n",
    "                          args=(training_sensitive, training_nonsensitive, \n",
    "                                ytrain_sensitive, ytrain_nonsensitive, k, 0.01,\n",
    "                                10, 50, 0),\n",
    "                          bounds = bnd, approx_grad=True, maxfun=15000, \n",
    "                          maxiter=15000)\n",
    "    \n",
    "    # extract values from obtained result vector\n",
    "    Ns, P = training_sensitive.shape\n",
    "    N,_=training_nonsensitive.shape\n",
    "    # alpha values for sensitive (0 after sklearn label encoder) and non-sensitive (1) groups.\n",
    "    alphaoptim0 = rez[0][:P]\n",
    "    alphaoptim1 = rez[0][P : 2 * P]\n",
    "    # weight vector\n",
    "    woptim = rez[0][2 * P : (2 * P) + k]\n",
    "    #cluster representatives\n",
    "    voptim = np.matrix(rez[0][(2 * P) + k:]).reshape((k, P))\n",
    "    \n",
    "    # learned distance function\n",
    "    dist_sensitive=distances(training_sensitive, voptim, alphaoptim1, Ns, P, k)\n",
    "    dist_nonsensitive=distances(training_nonsensitive,voptim,alphaoptim0,N,P,k)\n",
    "    \n",
    "    # learned cluster mapping probabilities\n",
    "    M_nk_sensitive=M_nk(dist_sensitive, Ns, k)\n",
    "    M_nk_nonsensitive=M_nk(dist_nonsensitive,N,k)\n",
    "    \n",
    "    # learned mappings\n",
    "    res_sensitive=x_n_hat(training_sensitive, M_nk_sensitive, voptim, Ns, P, k)\n",
    "    x_n_hat_sensitive=res_sensitive[0]\n",
    "    res_nonsensitive=x_n_hat(training_nonsensitive, M_nk_nonsensitive, voptim, N, P, k)\n",
    "    x_n_hat_nonsensitive=res_nonsensitive[0]\n",
    "    \n",
    "    # compute predictions for training dataset\n",
    "    res_sensitive=yhat(M_nk_sensitive, ytrain_sensitive, woptim, Ns, k)\n",
    "    y_hat_sensitive=res_sensitive[0]\n",
    "    res_nonsensitive=yhat(M_nk_nonsensitive, ytrain_nonsensitive, woptim, N, k)\n",
    "    y_hat_nonsensitive=res_nonsensitive[0]\n",
    "    \n",
    "    # preserve ordering (done here because of how the implementation is to minimize confusion for me!)\n",
    "    yordered=np.concatenate((ytrain_sensitive,ytrain_nonsensitive))\n",
    "    #ordered final train predictions\n",
    "    yhatordered=np.concatenate((y_hat_sensitive,y_hat_nonsensitive))\n",
    "    \n",
    "    traindataordered=np.concatenate((training_sensitive,training_nonsensitive))\n",
    "    # ordered final train mappings\n",
    "    traindatahatordered=np.concatenate((x_n_hat_sensitive,x_n_hat_nonsensitive))\n",
    "        \n",
    "    return originaldat,traindataordered,yordered,traindatahatordered,yhatordered,rez\n",
    "\n",
    "def ZemelPrediction(testing_sensitive,testing_nonsensitive,ytest_sensitive,ytest_nonsensitive,rez):\n",
    "    \"\"\"\n",
    "    This code uses the model learned from the training data and applies that model (rez) on the test data to obtain the final\n",
    "    yhat predictions (yhat=ytilde here as there is no separate prediction algorithm applied here). The resulting predictions\n",
    "    on test are returned.\n",
    "    \"\"\"\n",
    "    k=5\n",
    "    # extract training model parameters\n",
    "    Ns, P = testing_sensitive.shape\n",
    "    N,_=testing_nonsensitive.shape\n",
    "    alphaoptim0 = rez[0][:P]\n",
    "    alphaoptim1 = rez[0][P : 2 * P]\n",
    "    woptim = rez[0][2 * P : (2 * P) + k]\n",
    "    voptim = np.matrix(rez[0][(2 * P) + k:]).reshape((k, P))\n",
    "    \n",
    "    # compute distances on the test dataset using train model params\n",
    "    dist_sensitive=distances(testing_sensitive, voptim, alphaoptim1, Ns, P, k)\n",
    "    dist_nonsensitive=distances(testing_nonsensitive,voptim,alphaoptim0,N,P,k)\n",
    "    \n",
    "    #compute cluster probabilities for test instances\n",
    "    M_nk_sensitive=M_nk(dist_sensitive, Ns, k)\n",
    "    M_nk_nonsensitive=M_nk(dist_nonsensitive,N,k)\n",
    "    \n",
    "    #compute predictions for test instances\n",
    "    res_sensitive=yhat(M_nk_sensitive, ytest_sensitive, woptim, Ns, k)\n",
    "    y_hat_sensitive=res_sensitive[0]\n",
    "    res_nonsensitive=yhat(M_nk_nonsensitive, ytest_nonsensitive, woptim, N, k)\n",
    "    y_hat_nonsensitive=res_nonsensitive[0]\n",
    "    \n",
    "    # return ordered test response and test predictions \n",
    "    yordered=np.concatenate((ytest_sensitive,ytest_nonsensitive))\n",
    "    yhatordered=np.concatenate((y_hat_sensitive,y_hat_nonsensitive))\n",
    "    return yordered,yhatordered\n",
    "\n",
    "def ZemelTestPreprocessing(df_test_new):\n",
    "    \"\"\"\n",
    "    This code uses the model learned from the training data and applies that model (rez) on the test data to obtain the final\n",
    "    yhat predictions (yhat=ytilde here as there is no separate prediction algorithm applied here). The resulting predictions\n",
    "    on test are returned.\n",
    "    \"\"\"\n",
    "    encoded_data=df_test_new\n",
    "    catlabelsage=pd.get_dummies(encoded_data['Age (decade)'])\n",
    "    catlabelseducation=pd.get_dummies(encoded_data['Education Years'])\n",
    "    encoded_data=pd.concat([encoded_data,catlabelsage],axis=1)\n",
    "    encoded_data=encoded_data.drop(['Age (decade)'],axis=1)\n",
    "    encoded_data=pd.concat([encoded_data,catlabelseducation],axis=1)\n",
    "    encoded_data=encoded_data.drop(['Education Years'],axis=1)\n",
    "    encoded_data=encoded_data.drop(['Income'],axis=1)\n",
    "    encoders = {}\n",
    "    encoders['Gender'] = preprocessing.LabelEncoder()\n",
    "    encoded_data['Gender'] = encoders['Gender'].fit_transform(encoded_data['Gender'])\n",
    "    sensitive_idx = np.array(np.where(encoded_data['Gender']==0))[0].flatten()\n",
    "    nonsensitive_idx = np.array(np.where(encoded_data['Gender']!=0))[0].flatten()\n",
    "    original_sensitive=df_test_new.iloc[sensitive_idx,:]\n",
    "    original_nonsensitive=df_test_new.iloc[nonsensitive_idx,:]\n",
    "    originaldat=pd.concat((original_sensitive,original_nonsensitive))\n",
    "    ytest_sensitive=np.array(encoded_data['Income Binary'])[sensitive_idx]\n",
    "    ytest_nonsensitive=np.array(encoded_data['Income Binary'])[nonsensitive_idx]\n",
    "    encoded_data=encoded_data.drop('Income Binary',axis=1)\n",
    "    testdat=np.array(encoded_data)\n",
    "    testing_sensitive=testdat[sensitive_idx,:]\n",
    "    testing_nonsensitive=testdat[nonsensitive_idx,:]\n",
    "    return testing_sensitive,testing_nonsensitive,ytest_sensitive,ytest_nonsensitive,originaldat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path =r'../experiment_data1/'\n",
    "#These are the test and train datasets by splitting the original data.\n",
    "train_0 = pd.read_csv(path + \"train_0.csv\",index_col=None, header=0, usecols=range(1,6))\n",
    "test_0 = pd.read_csv(path + \"test_0.csv\",index_col=None, header=0, usecols=range(1,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhanu/anaconda3/envs/python2/lib/python2.7/site-packages/numba/dataflow.py:346: RuntimeWarning: Python2 style print partially supported.  Please use Python3 style print.\n",
      "  \"Python3 style print.\", RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250, 208086.713327976)\n",
      "\n",
      "(500, 146632.3224609308)\n",
      "\n",
      "(750, 140607.9096274308)\n",
      "\n",
      "(1000, 133559.8156723103)\n",
      "\n",
      "(1250, 127437.95759365942)\n",
      "\n",
      "(1500, 120929.84266487334)\n",
      "\n",
      "(1750, 119210.29961840365)\n",
      "\n",
      "(2000, 118556.72138301308)\n",
      "\n",
      "(2250, 116239.55064816067)\n",
      "\n",
      "(2500, 115691.7587257128)\n",
      "\n",
      "(2750, 115441.03806050186)\n",
      "\n",
      "(3000, 115262.47301654513)\n",
      "\n",
      "(3250, 114583.76035506575)\n",
      "\n",
      "(3500, 113728.20937986134)\n",
      "\n",
      "(3750, 113344.40410204856)\n",
      "\n",
      "(4000, 112982.89439690876)\n",
      "\n",
      "(4250, 112924.08227246214)\n",
      "\n",
      "(4500, 112705.70435687459)\n",
      "\n",
      "(4750, 112180.98434815403)\n",
      "\n",
      "(5000, 112031.08944009314)\n",
      "\n",
      "(5250, 111715.75283357836)\n",
      "\n",
      "(5500, 111451.04010434488)\n",
      "\n",
      "(5750, 111316.15902916776)\n",
      "\n",
      "(6000, 111297.85124703211)\n",
      "\n",
      "(6250, 111240.1907520863)\n",
      "\n",
      "(6500, 111105.18917020709)\n",
      "\n",
      "(6750, 111064.35487114808)\n",
      "\n",
      "(7000, 111033.35463693192)\n",
      "\n",
      "(7250, 110972.37970249914)\n",
      "\n",
      "(7500, 110968.37812229582)\n",
      "\n",
      "(7750, 110907.84952208665)\n",
      "\n",
      "(8000, 110903.74589356229)\n",
      "\n",
      "(8250, 110865.84947559901)\n",
      "\n",
      "(8500, 110852.75875561213)\n",
      "\n",
      "(8750, 110833.75247505863)\n",
      "\n",
      "(9000, 110788.76097267689)\n",
      "\n",
      "(9250, 110782.12968491456)\n",
      "\n",
      "(9500, 110763.78822066588)\n",
      "\n",
      "(9750, 110748.82783395065)\n",
      "\n",
      "(10000, 110720.20279674052)\n",
      "\n",
      "(10250, 110684.98321263229)\n",
      "\n",
      "(10500, 110670.08126607974)\n",
      "\n",
      "(10750, 110669.1789254417)\n",
      "\n",
      "(11000, 110664.0716315785)\n",
      "\n",
      "(11250, 110684.34933122103)\n",
      "\n",
      "(11500, 110650.60976918608)\n",
      "\n",
      "(11750, 110641.69372203927)\n",
      "\n",
      "(12000, 110649.09636139714)\n",
      "\n",
      "(12250, 110636.45025876186)\n",
      "\n",
      "(12500, 110634.79734592054)\n",
      "\n",
      "(12750, 110633.17943542659)\n",
      "\n",
      "(13000, 110921.2047194577)\n",
      "\n",
      "(13250, 110629.52062469815)\n",
      "\n",
      "(13500, 110624.96334347743)\n",
      "\n",
      "(13750, 110623.19722634381)\n",
      "\n",
      "(14000, 110621.17819904209)\n",
      "\n",
      "(14250, 110619.86080336716)\n",
      "\n",
      "(14500, 110615.20942331733)\n",
      "\n",
      "(14750, 110612.19457208047)\n",
      "\n",
      "(15000, 110610.0479460346)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "zemel_df_train_res=ZemelTransform(train_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "Zemel algorithm (ICML 2013) performance:\n",
      "Train performance with pert. dataset: \n",
      "0.821354664277\n",
      "Perturbed test performance when scored on original test y variable: \n",
      "0.819043900593\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.6772310389115086"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from __future__ import division\n",
    "\n",
    "print '----------------------------------------------------------------'\n",
    "print 'Zemel algorithm (ICML 2013) performance:'\n",
    "response=zemel_df_train_res[2]\n",
    "pred=zemel_df_train_res[4]\n",
    "print 'Train performance with pert. dataset: '\n",
    "print roc_auc_score(response,pred)\n",
    "\n",
    "zemel_preprocess=ZemelTestPreprocessing(test_0)\n",
    "zemel_prediction= ZemelPrediction(zemel_preprocess[0],zemel_preprocess[1],zemel_preprocess[2],zemel_preprocess[3],zemel_df_train_res[5])\n",
    "print 'Perturbed test performance when scored on original test y variable: '\n",
    "print roc_auc_score(zemel_prediction[0],zemel_prediction[1])\n",
    "\n",
    "# save performance\n",
    "df_test_pred = zemel_preprocess[4]\n",
    "df_test_pred['pred'] = zemel_prediction[1]\n",
    "\n",
    "mean = df_test_pred.groupby('Gender')['pred'].mean()\n",
    "v = mean.values\n",
    "v = v.reshape(len(v),1)\n",
    "ratio_df = pd.DataFrame(v/v.transpose(),index=mean.index,columns=mean.index )\n",
    "ratio_df_arr=np.asarray(np.abs(1-ratio_df))\n",
    "zemel_discrim=np.amax(ratio_df_arr)\n",
    "zemel_discrim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
