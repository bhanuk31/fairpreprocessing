{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import scipy.optimize as optim\n",
    "from helpers import *\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ZemelTransform(encoded_data):\n",
    "    \"\"\"\n",
    "    This function learns the model from the training set for Adult data which is binarized (in the steps below) and passed to the LBFGS solver\n",
    "    after computing the composite function using helpers.py. helpers.py has all the functions used in Zemel's paper as a s\n",
    "    separate module. As given in the paper the parameter setting (0.01,1,50) with\n",
    "    a k=10 gives good accuracy and reasonable discrimination for Adult dataset. Another setting of (0.01,0.01,0) gives low\n",
    "    discrimination. k value can be modified in the code below or passed as a parameter.\n",
    "    \"\"\"\n",
    "    k=5\n",
    "    df_train=encoded_data\n",
    "    catlabelsage=pd.get_dummies(encoded_data['Age (decade)'])\n",
    "    catlabelseducation=pd.get_dummies(encoded_data['Education Years'])\n",
    "    encoded_data=pd.concat([encoded_data,catlabelsage],axis=1)\n",
    "    encoded_data=encoded_data.drop(['Age (decade)'],axis=1)\n",
    "    encoded_data=pd.concat([encoded_data,catlabelseducation],axis=1)\n",
    "    encoded_data=encoded_data.drop(['Education Years'],axis=1)\n",
    "    encoded_data=encoded_data.drop(['Income'],axis=1)\n",
    "    encoders = {}\n",
    "    encoders['Gender'] = preprocessing.LabelEncoder()\n",
    "    encoded_data['Gender'] = encoders['Gender'].fit_transform(encoded_data['Gender'])\n",
    "    sensitive_idx = np.array(np.where(encoded_data['Gender']==0))[0].flatten()\n",
    "    nonsensitive_idx = np.array(np.where(encoded_data['Gender']!=0))[0].flatten()\n",
    "    original_sensitive=df_train.iloc[sensitive_idx,:]\n",
    "    original_nonsensitive=df_train.iloc[nonsensitive_idx,:]\n",
    "    originaldat=pd.concat((original_sensitive,original_nonsensitive))\n",
    "    ytrain_sensitive=np.array(encoded_data['Income Binary'])[sensitive_idx]\n",
    "    ytrain_nonsensitive=np.array(encoded_data['Income Binary'])[nonsensitive_idx]\n",
    "    encoded_data=encoded_data.drop('Income Binary',axis=1)\n",
    "    traindat=np.array(encoded_data)\n",
    "    training_sensitive=traindat[sensitive_idx,:]\n",
    "    training_nonsensitive=traindat[nonsensitive_idx,:]\n",
    "    rez = np.random.uniform(size=traindat.shape[1] * 2 + k + traindat.shape[1] * k)\n",
    "    bnd = []\n",
    "    for i, k2 in enumerate(rez):\n",
    "        if i < traindat.shape[1] * 2 or i >= traindat.shape[1] * 2 + k:\n",
    "            bnd.append((None, None))\n",
    "        else:\n",
    "            bnd.append((0, 1))\n",
    "    rez = optim.fmin_l_bfgs_b(LFR, x0=rez, epsilon=1e-5, \n",
    "                          args=(training_sensitive, training_nonsensitive, \n",
    "                                ytrain_sensitive, ytrain_nonsensitive, k, 0.01,\n",
    "                                1, 50, 0),\n",
    "                          bounds = bnd, approx_grad=True, maxfun=15000, \n",
    "                          maxiter=15000)\n",
    "    \n",
    "    # extract values from obtained result vector\n",
    "    Ns, P = training_sensitive.shape\n",
    "    N,_=training_nonsensitive.shape\n",
    "    # alpha values for sensitive (0 after sklearn label encoder) and non-sensitive (1) groups.\n",
    "    alphaoptim0 = rez[0][:P]\n",
    "    alphaoptim1 = rez[0][P : 2 * P]\n",
    "    # weight vector\n",
    "    woptim = rez[0][2 * P : (2 * P) + k]\n",
    "    #cluster representatives\n",
    "    voptim = np.matrix(rez[0][(2 * P) + k:]).reshape((k, P))\n",
    "    \n",
    "    # learned distance function\n",
    "    dist_sensitive=distances(training_sensitive, voptim, alphaoptim1, Ns, P, k)\n",
    "    dist_nonsensitive=distances(training_nonsensitive,voptim,alphaoptim0,N,P,k)\n",
    "    \n",
    "    # learned cluster mapping probabilities\n",
    "    M_nk_sensitive=M_nk(dist_sensitive, Ns, k)\n",
    "    M_nk_nonsensitive=M_nk(dist_nonsensitive,N,k)\n",
    "    \n",
    "    # learned mappings\n",
    "    res_sensitive=x_n_hat(training_sensitive, M_nk_sensitive, voptim, Ns, P, k)\n",
    "    x_n_hat_sensitive=res_sensitive[0]\n",
    "    res_nonsensitive=x_n_hat(training_nonsensitive, M_nk_nonsensitive, voptim, N, P, k)\n",
    "    x_n_hat_nonsensitive=res_nonsensitive[0]\n",
    "    \n",
    "    # compute predictions for training dataset\n",
    "    res_sensitive=yhat(M_nk_sensitive, ytrain_sensitive, woptim, Ns, k)\n",
    "    y_hat_sensitive=res_sensitive[0]\n",
    "    res_nonsensitive=yhat(M_nk_nonsensitive, ytrain_nonsensitive, woptim, N, k)\n",
    "    y_hat_nonsensitive=res_nonsensitive[0]\n",
    "    \n",
    "    # preserve ordering (done here because of how the implementation is to minimize confusion for me!)\n",
    "    yordered=np.concatenate((ytrain_sensitive,ytrain_nonsensitive))\n",
    "    #ordered final train predictions\n",
    "    yhatordered=np.concatenate((y_hat_sensitive,y_hat_nonsensitive))\n",
    "    \n",
    "    traindataordered=np.concatenate((training_sensitive,training_nonsensitive))\n",
    "    # ordered final train mappings\n",
    "    traindatahatordered=np.concatenate((x_n_hat_sensitive,x_n_hat_nonsensitive))\n",
    "        \n",
    "    return originaldat,traindataordered,yordered,traindatahatordered,yhatordered,rez\n",
    "\n",
    "def ZemelPrediction(testing_sensitive,testing_nonsensitive,ytest_sensitive,ytest_nonsensitive,rez):\n",
    "    \"\"\"\n",
    "    This code uses the model learned from the training data and applies that model (rez) on the test data to obtain the final\n",
    "    yhat predictions (yhat=ytilde here as there is no separate prediction algorithm applied here). The resulting predictions\n",
    "    on test are returned.\n",
    "    \"\"\"\n",
    "    k=5\n",
    "    # extract training model parameters\n",
    "    Ns, P = testing_sensitive.shape\n",
    "    N,_=testing_nonsensitive.shape\n",
    "    alphaoptim0 = rez[0][:P]\n",
    "    alphaoptim1 = rez[0][P : 2 * P]\n",
    "    woptim = rez[0][2 * P : (2 * P) + k]\n",
    "    voptim = np.matrix(rez[0][(2 * P) + k:]).reshape((k, P))\n",
    "    \n",
    "    # compute distances on the test dataset using train model params\n",
    "    dist_sensitive=distances(testing_sensitive, voptim, alphaoptim1, Ns, P, k)\n",
    "    dist_nonsensitive=distances(testing_nonsensitive,voptim,alphaoptim0,N,P,k)\n",
    "    \n",
    "    #compute cluster probabilities for test instances\n",
    "    M_nk_sensitive=M_nk(dist_sensitive, Ns, k)\n",
    "    M_nk_nonsensitive=M_nk(dist_nonsensitive,N,k)\n",
    "    \n",
    "    #compute predictions for test instances\n",
    "    res_sensitive=yhat(M_nk_sensitive, ytest_sensitive, woptim, Ns, k)\n",
    "    y_hat_sensitive=res_sensitive[0]\n",
    "    res_nonsensitive=yhat(M_nk_nonsensitive, ytest_nonsensitive, woptim, N, k)\n",
    "    y_hat_nonsensitive=res_nonsensitive[0]\n",
    "    \n",
    "    # return ordered test response and test predictions \n",
    "    yordered=np.concatenate((ytest_sensitive,ytest_nonsensitive))\n",
    "    yhatordered=np.concatenate((y_hat_sensitive,y_hat_nonsensitive))\n",
    "    return yordered,yhatordered\n",
    "\n",
    "def ZemelTestPreprocessing(df_test_new):\n",
    "    \"\"\"\n",
    "    This code uses the model learned from the training data and applies that model (rez) on the test data to obtain the final\n",
    "    yhat predictions (yhat=ytilde here as there is no separate prediction algorithm applied here). The resulting predictions\n",
    "    on test are returned.\n",
    "    \"\"\"\n",
    "    encoded_data=df_test_new\n",
    "    catlabelsage=pd.get_dummies(encoded_data['Age (decade)'])\n",
    "    catlabelseducation=pd.get_dummies(encoded_data['Education Years'])\n",
    "    encoded_data=pd.concat([encoded_data,catlabelsage],axis=1)\n",
    "    encoded_data=encoded_data.drop(['Age (decade)'],axis=1)\n",
    "    encoded_data=pd.concat([encoded_data,catlabelseducation],axis=1)\n",
    "    encoded_data=encoded_data.drop(['Education Years'],axis=1)\n",
    "    encoded_data=encoded_data.drop(['Income'],axis=1)\n",
    "    encoders = {}\n",
    "    encoders['Gender'] = preprocessing.LabelEncoder()\n",
    "    encoded_data['Gender'] = encoders['Gender'].fit_transform(encoded_data['Gender'])\n",
    "    sensitive_idx = np.array(np.where(encoded_data['Gender']==0))[0].flatten()\n",
    "    nonsensitive_idx = np.array(np.where(encoded_data['Gender']!=0))[0].flatten()\n",
    "    original_sensitive=df_test_new.iloc[sensitive_idx,:]\n",
    "    original_nonsensitive=df_test_new.iloc[nonsensitive_idx,:]\n",
    "    originaldat=pd.concat((original_sensitive,original_nonsensitive))\n",
    "    ytest_sensitive=np.array(encoded_data['Income Binary'])[sensitive_idx]\n",
    "    ytest_nonsensitive=np.array(encoded_data['Income Binary'])[nonsensitive_idx]\n",
    "    encoded_data=encoded_data.drop('Income Binary',axis=1)\n",
    "    testdat=np.array(encoded_data)\n",
    "    testing_sensitive=testdat[sensitive_idx,:]\n",
    "    testing_nonsensitive=testdat[nonsensitive_idx,:]\n",
    "    return testing_sensitive,testing_nonsensitive,ytest_sensitive,ytest_nonsensitive,originaldat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26048, 5)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path =r'../experiment_data1/'\n",
    "#These are the test and train datasets by splitting the original data.\n",
    "train_0 = pd.read_csv(path + \"train_0.csv\",index_col=None, header=0, usecols=range(1,6))\n",
    "test_0 = pd.read_csv(path + \"test_0.csv\",index_col=None, header=0, usecols=range(1,6))\n",
    "\n",
    "train_0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhanu/anaconda3/envs/python2/lib/python2.7/site-packages/numba/dataflow.py:346: RuntimeWarning: Python2 style print partially supported.  Please use Python3 style print.\n",
      "  \"Python3 style print.\", RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250, 17308.859644389097)\n",
      "\n",
      "(500, 15236.493871300623)\n",
      "\n",
      "(750, 13803.67692730774)\n",
      "\n",
      "(1000, 13614.82064304608)\n",
      "\n",
      "(1250, 12739.54254332751)\n",
      "\n",
      "(1500, 12446.45174810471)\n",
      "\n",
      "(1750, 12302.737130250951)\n",
      "\n",
      "(2000, 12057.127614571888)\n",
      "\n",
      "(2250, 11986.998719936026)\n",
      "\n",
      "(2500, 11837.189582599587)\n",
      "\n",
      "(2750, 11778.405433248709)\n",
      "\n",
      "(3000, 11731.132897569007)\n",
      "\n",
      "(3250, 11673.71556016283)\n",
      "\n",
      "(3500, 11661.371236842282)\n",
      "\n",
      "(3750, 11635.591178552817)\n",
      "\n",
      "(4000, 11593.64605763025)\n",
      "\n",
      "(4250, 11577.474722048915)\n",
      "\n",
      "(4500, 11566.815507085354)\n",
      "\n",
      "(4750, 11556.168273097446)\n",
      "\n",
      "(5000, 11540.495084023085)\n",
      "\n",
      "(5250, 11530.69437128558)\n",
      "\n",
      "(5500, 11528.589537316311)\n",
      "\n",
      "(5750, 11521.349653729309)\n",
      "\n",
      "(6000, 11517.110823428247)\n",
      "\n",
      "(6250, 11511.087930392725)\n",
      "\n",
      "(6500, 11502.595023811515)\n",
      "\n",
      "(6750, 11500.113926863254)\n",
      "\n",
      "(7000, 11498.63749901459)\n",
      "\n",
      "(7250, 11497.141958848173)\n",
      "\n",
      "(7500, 11494.21346902748)\n",
      "\n",
      "(7750, 11490.303127262248)\n",
      "\n",
      "(8000, 11485.994663410742)\n",
      "\n",
      "(8250, 11484.486876576677)\n",
      "\n",
      "(8500, 11482.92312941706)\n",
      "\n",
      "(8750, 11480.332966775564)\n",
      "\n",
      "(9000, 11478.921576802615)\n",
      "\n",
      "(9250, 11475.627317997867)\n",
      "\n",
      "(9500, 11472.891752198995)\n",
      "\n",
      "(9750, 11471.481059059286)\n",
      "\n",
      "(10000, 11469.2692345955)\n",
      "\n",
      "(10250, 11468.417381516352)\n",
      "\n",
      "(10500, 11466.170423748477)\n",
      "\n",
      "(10750, 11464.195656491813)\n",
      "\n",
      "(11000, 11463.563771248935)\n",
      "\n",
      "(11250, 11462.492963097975)\n",
      "\n",
      "(11500, 11460.254788417647)\n",
      "\n",
      "(11750, 11458.579202262326)\n",
      "\n",
      "(12000, 11456.94238788272)\n",
      "\n",
      "(12250, 11456.42833166801)\n",
      "\n",
      "(12500, 11455.916363271795)\n",
      "\n",
      "(12750, 11454.65733489382)\n",
      "\n",
      "(13000, 11454.08082779444)\n",
      "\n",
      "(13250, 11454.006726002946)\n",
      "\n",
      "(13500, 11453.866056326713)\n",
      "\n",
      "(13750, 11453.504045695536)\n",
      "\n",
      "(14000, 11453.44330982088)\n",
      "\n",
      "(14250, 11452.884192400043)\n",
      "\n",
      "(14500, 11452.293492769391)\n",
      "\n",
      "(14750, 11452.086397120192)\n",
      "\n",
      "(15000, 11451.94457125948)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "zemel_df_train_res=ZemelTransform(train_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "Zemel algorithm (ICML 2013) performance:\n",
      "Train performance with pert. dataset: \n",
      "0.821456810411\n",
      "Perturbed test performance when scored on original test y variable: \n",
      "0.818830165357\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.6781477431939105"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from __future__ import division\n",
    "\n",
    "print '----------------------------------------------------------------'\n",
    "print 'Zemel algorithm (ICML 2013) performance:'\n",
    "response=zemel_df_train_res[2]\n",
    "pred=zemel_df_train_res[4]\n",
    "print 'Train performance with pert. dataset: '\n",
    "print roc_auc_score(response,pred)\n",
    "\n",
    "zemel_preprocess=ZemelTestPreprocessing(test_0)\n",
    "zemel_prediction= ZemelPrediction(zemel_preprocess[0],zemel_preprocess[1],zemel_preprocess[2],zemel_preprocess[3],zemel_df_train_res[5])\n",
    "print 'Perturbed test performance when scored on original test y variable: '\n",
    "print roc_auc_score(zemel_prediction[0],zemel_prediction[1])\n",
    "\n",
    "# save performance\n",
    "df_test_pred = zemel_preprocess[4]\n",
    "df_test_pred['pred'] = zemel_prediction[1]\n",
    "\n",
    "mean = df_test_pred.groupby('Gender')['pred'].mean()\n",
    "v = mean.values\n",
    "v = v.reshape(len(v),1)\n",
    "ratio_df = pd.DataFrame(v/v.transpose(),index=mean.index,columns=mean.index )\n",
    "ratio_df_arr=np.asarray(np.abs(1-ratio_df))\n",
    "zemel_discrim=np.amax(ratio_df_arr)\n",
    "zemel_discrim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = df_test_pred.groupby('Gender')['pred'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4372, 17)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zemel_preprocess[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
